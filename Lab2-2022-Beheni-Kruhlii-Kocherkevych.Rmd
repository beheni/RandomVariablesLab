---
title: 'P&S-2022: Lab assignment 2'
author: "Victoriia Kocherkevych, Markiian-Mykolai Kruhlii, Beheni Anastasiia"
output:
  html_document:
    df_print: paged
---

### Task 1 (Anastasiia Beheni)

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
id <- 19                  
set.seed(id)
p <- id/100

# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
print("The matrix G is: \n")
G  
print("The matrix H is: \n")
H
print("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(1000)
codewords <- (messages %*% G) %% 2
#messages
#codewords
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
#generates matrix of error for each message 1 - the bit will be wrong
generate_errors <- function(N){
  matrix(sample(0:1, 7*N, replace=T, prob=c(1-p,p)), nrow = N)
}

print("codewords:")
codewords[(1:10),]

errors <- generate_errors(1000)
print("errors:")
errors[(1:10),]
```

```{r}
#corrupting the original messages
received <- (codewords + errors) %% 2
print("received:")
received[(1:10),]

#finding the syndrome vector for finding the bit with mistake
syndrome_vector <- (received %*% H) %% 2
#print("syndrome vector")
#syndrome_vector[(1: 10), ]

#gets number of the bit with mistake using syndrome vector
get_number_bit = function(M){
  z1 = M[1]
  z2 = M[2]
  z3 = M[3]
  return(z1+2*z2+4*z3)
}

#finding the bit with error for each received message
number_bit_with_error <- apply(syndrome_vector, 1, get_number_bit)
print("number bit with error")
number_bit_with_error[(1:10)]


received_corrected = received
#correcting the bit with error by just reversing it
for(row in 1:nrow(received_corrected)) {
    received_corrected[row, number_bit_with_error[row]] = (received_corrected[row, number_bit_with_error[row]]+1)%%2
}
print("received corrected:")
received_corrected[(1:10),]
```

```{r}
#decoding the message by deleating all parity bits
decoded <- cbind(received_corrected[,3], received_corrected[,5], received_corrected[,6], received_corrected[,7])
print("messages:")
messages[(1:10),]
print("decoded")
decoded[(1:10), ]

#finding the sum of wrongly decoded messages
p_hat <- sum(
  decoded[,1]==messages[,1] & 
  decoded[,2]==messages[,2]&
  decoded[,3]==messages[,3]&
  decoded[,4]==messages[,4]
  ) / nrow(decoded)
print("p hat")
p_hat
```

Experimentally we found the estimation $\hat p$ of $p^*$ which is the probability of correctly transmitted message.\
The message is transmitted correctly when zero bits were corrupted or only one bit was corrupted. One bit will be corrupted with probability $p=\frac{19}{100}=0.19$\
$p^* = (1-p)^7 + {7\choose 1}(1-p)^6p^1=(\frac{81}{100})^7 + 7\cdot(\frac{81}{100})^6*(\frac{19}{100})\approx0.604$

### Why $\hat p$ is expected to be close to $p^*$ for large $N$?

We have $\hat p$ is the sum of random indicators of success (in our case success == correctly decoded message)\
$$I = \begin{cases}
1 & p^*\\
0 & 1-p^*
\end{cases}$$ $$E(I) = 1*p^*+0*(1-p^*) = p^*$$ $E(I^2) = 1^2*p^*+0^2*(1-p^*) = p^*$ $$Var(I) = E(I^2)-E(I)^2 =  p^*- (p^*)^2$$

Because of the **Weak Law of Large Numbers**, we can determine that $P(|\hat p - p^* \ge \epsilon|) \to 0$ \### Confidence interval The standart deviation of our indicator r. v. $$\sigma = \sqrt{Var(I)} = \sqrt{p^*- (p^*)^2} \approx \sqrt{0.604-0.604^2} = 0.489$$

Using the **Central Limit Theorem** we know that $Z=\frac{\sqrt{N}}{\sigma}(\hat{p}-p^*)=\frac{\sqrt{N}}{0.489}(\hat{p}-0.604)$ is a standard normal r. v. Therefore\
$$P(\hat p -\epsilon <p^*<\hat p + \epsilon)= P(-\epsilon<p^*-\hat{p}<\epsilon)=P(-\epsilon<-(\hat{p}-p^*)<\epsilon)=\\P(\frac{-\epsilon\sqrt{N}}{\sigma}<\frac{-\epsilon\sqrt{N}}{\sigma}(\hat{p}-p^*)<\frac{\epsilon\sqrt{N}}{\sigma})= P(\frac{-\epsilon\sqrt{N}}{0.489}<\frac{-\epsilon\sqrt{N}}{0.489}(\hat{p}-p^*)<\frac{\epsilon\sqrt{N}}{0.489})=\\=\Phi(\frac{\epsilon\sqrt{N}}{0.489})-\Phi(\frac{-\epsilon\sqrt{N}}{0.489})=2\Phi(\frac{\epsilon\sqrt{N}}{0.489})-1=0.95$$ $\Phi(\frac{\epsilon\sqrt{N}}{0.489}) = \frac{0.95+1}{2} = 0.975$ Using the Standard normal table we got that $\frac{\epsilon\sqrt{N}}{0.489} = 1.96$ Thus $$\epsilon = \frac{1.96*0.489}{\sqrt{N}} = \frac{0.958}{\sqrt{N}}$$ This is equal to $\approx 0,09$ when $N=100$\
To satisfy the condition that $\epsilon \le 0,03$\
$$N\ge (\frac{0.958}{0.03})^2\ge 1020$$ \### Historgam of Errors Amount

```{r}
#calculating the number of mistakes in each message
number_of_mistakes = c()
print(number_of_mistakes)
for(row in 1:nrow(messages)) {
  i = 0
    for(col in 1:ncol(messages)) {
      if (messages[row, col] != decoded[row, col])
        i = i+1
    }
  number_of_mistakes<-append(number_of_mistakes, i)
}

#making the histogram
hist(number_of_mistakes, xlim = c(0,4), col="purple", xlab = "Number of mistakes", main="Number of mistakes and its frequency")

```

The random variable that counts the number of wrong bits in a decoded message seems to have a binomial distribution. But in fact **it doesn't have a known distribution**. Because we can fix one mistake and we can't predict what will be decoded if we have more than one error in the message, because *[7, 4] Hamming code can't deal with it*.\

**In conclusion**, we have the probability of corrupting one bit while transmitting. But we *can't predict the probability of corrupting one bit while decoding*, because with more than two mistakes we are reversing an undetermined bit.

### Task 2.

#### In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

#### Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

#### Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
lambda <- 1  # change this!
N <- 100     # change this!
mu <- N * lambda
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

## Part 1

#### First, generate samples an sample means:

```{r}
nu1 <- id + 10
K <- 1e3
ns <- c(5, 10, 50)
sample_means <- colMeans(matrix(rexp(5*K, rate = nu1), nrow=5))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
#mu <- 1 / nu1
#sigma <- mu / sqrt(n)
```

#### We can now plot ecdf and cdf

```{r}
x <- seq(0,0.200,by=0.001)

for (n in ns) {
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
  
  mu <- 1 / nu1
  sigma <- mu / sqrt(n)
  
  xlims <- c(mu-3*sigma,mu+3*sigma)
  Fs <- ecdf(sample_means)
  
  title <- paste("ecdf and cdf for n = ", toString(n))
  
  plot(Fs, 
       xlim = xlims, 
       col = "blue",
       lwd = 2,
       main = title)
  
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  
  print(max(abs(ecdf(sample_means)(x)-pnorm(x,mean = mu, sd = sigma))))
}
```
## Part 2

$S = X_1 + ... + X_{100}$

$P(S>=60)$

**Markov**
$$
P(S \ge 1) \le \frac {\mu_S}{1} = \frac {100} {29*N}
$$
$$
0.95 \le \frac {100} {29*N}
$$
$$N=3$$

**Chebyshev**

$$
P(|X - \mu| \ge c) \le \frac {\sigma^2}{c^2} \Rightarrow P(S - \frac {100} {29*N} \ge 1-  \frac {100} {29*N})
$$

$$
P(|S - \frac {100} {29*N}| \ge \frac {29*N -100} {29*N}) \le \frac {100*N^2} {(29 * N - 100) ^ 2}
$$

$$
0.95 \le \frac {100*N^2} {(24 * N - 100) ^ 2}
$$

$$
N = 3
$$

**Central Limit theorem:**

$$
P(\frac {S - 100 * \mu} {\sigma * \sqrt n} \le t) \rightarrow Ф(t)
$$

$$
\mu_S = \frac {100} {29*N} = \sigma_S
$$
$$
P(S\ge1) = P(\frac {S - 100 * \frac 1 {29 * N}}{10 * \frac 1 {29 * N}} \le \frac {1 - 100 * \frac 1 {29 * N}}{10 * \frac 1 {29 * N}}) = Ф(\frac {29 * N- 100} {10})
$$
$$
Ф(\frac {24 * N- 100} {10}) \ge 0.95
$$
Lets estimate $N$ using R inbuild tools

```{r}
x <- 1:5
results <- c()
for (N in seq(min(x), max(x), len=1000)) {
  res = pnorm((nu1 * N - 100) / 10)
  
  if (res >= 0.95 && res <= 1) {
    results <- append(results, c(res, N))
  }
}
max(results)
```
$N=5$


```{r}

N <- 3
nu2 = nu1 * N
sample = replicate(1000, sum(rexp(100, rate = nu)))
probability = sum(sample > 1) / 1000
print("Probability that the location is identified as safe")
probability
```

### General summary and conclusions

SUmmarize here what you've done, whether you solved the tasks, what difficulties you had etc
